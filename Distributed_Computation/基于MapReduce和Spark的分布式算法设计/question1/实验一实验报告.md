# 实验一

## 实验内容

题目1 输入文件为学生成绩信息，包含了必修课与选修课成绩，格式如下： 

班级1, 姓名1, 科目1, 必修, 成绩1 <br>班级2, 姓名2, 科目1, 必修, 成绩2 <br>班级1, 姓名1, 科目2, 选修, 成绩3 <br> ………., ………, ………, ………   <br> 编写两个Hadoop平台上的MapReduce程序，分别实现如下功能： 1. 计算每个学生必修课的平均成绩。 2. 按科目统计每个班的平均成绩。 

### commands

**MapReduceStuAvg**

```powershell
cd ./share/question1/MapReduceStuAvg

mvn clean
mvn package

hadoop fs -mkdir /input
hadoop fs -put ./grades.txt /input
hadoop fs -cat /input/grades.txt

hadoop jar ./target/MapReduceStuAvg.jar org.example.MapReduceStuAvg /input/grades.txt /output

hadoop fs -ls /output
hadoop fs -cat /output/part-r-00000
hadoop fs -rm -r /output
```

**MapReduceClaAvg**

```powershell
cd ./share/question1/MapReduceClaAvg

mvn clean
mvn package

hadoop fs -mkdir /input
hadoop fs -put ./grades.txt /input
hadoop fs -cat /input/grades.txt

hadoop jar ./target/MapReduceClaAvg.jar org.example.MapReduceClaAvg /input/grades.txt /output

hadoop fs -ls /output
hadoop fs -cat /output/part-r-00000
hadoop fs -rm -r /output
```



## 设计思想

MapReduce并行计算模型的计算任务主要划分为三个阶段：Map阶段，聚集混洗阶段、Reduce阶段。

### 任务一

任务一要求计算每个学生必修课的平均成绩，所以在map阶段首先要过滤掉选修课的数据;

#### map

```java
public static class MyMapper extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, Text> {//输入的key，value；输出的key，value类型
        Text KeyOut = new Text();
        Text ValueOut = new Text();

        @Override
        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context)
                throws IOException, InterruptedException {
            String line = value.toString();
            String[] splited = line.split(",");
            //通过判断课程类型，来过滤掉选修的数据
            if (splited[3].equals("必修")) {
                KeyOut.set(splited[1]); //name
                ValueOut.set(splited[4]); //score
                context.write(KeyOut, ValueOut);
            }
        }
    }
```

mappe阶段，输出的<key,value> 为

```
key:name
value:score
```

在聚集混洗阶段，不同Map子任务输出的<key,value>数组按照**key值**进行聚集。

#### reduce

对于reduce阶段，输入的<key,values>为

```
key：name
values：某一学生对应的所有必修课分数score的一个迭代器
```

```java
public static class MyReducer extends org.apache.hadoop.mapreduce.Reducer<Text, Text, Text, Text> {
        Text ValueOut = new Text();
        List<Integer> scoreList = new ArrayList<>();

        @Override
        protected void reduce(Text k2, Iterable<Text> v2s,
                              Reducer<Text, Text, Text, Text>.Context context) throws IOException, InterruptedException {
            scoreList.clear();
            for (Text v2 : v2s) {
                scoreList.add(Integer.valueOf(v2.toString()));
            }

            int sum_score = 0;
            for (int score : scoreList) {
                sum_score += score;
            }

            double avg_score = sum_score * 1D / scoreList.size();

            ValueOut.set(String.valueOf(avg_score));
            context.write(k2, ValueOut);
        }
    }
```

​	根据题目需求计算每一个学生必修课的平均成绩，即对每一个key-value键值对，对value迭代求总和再取均值即可。

### 任务二

任务二要求按科目统计每个班的平均成绩，所以在map阶段将科目与班级拼接作为key，对应的score作为value；

#### map

```java
public static class MyMapper extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, Text>{//输入的key，value；输出的key，value类型
        Text KeyOut = new Text();
        Text ValueOut = new Text();
        @Override
        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context)
                throws IOException, InterruptedException {
            String line = value.toString();
            String[] splited = line.split(",");
            String class_no = splited[0]; //班级
            String subject = splited[2]; //科目
            String score = splited[4];  //分数
            KeyOut.set(subject+'\t'+class_no+'\t');  //将科目与班级拼接起来作为key
            ValueOut.set(score);

            context.write(KeyOut,ValueOut);
        }
    }
```

mappe阶段，输出的<key,value> 为

```
key:subject+class
value:score
```

在聚集混洗阶段，不同Map子任务输出的<key,value>数组按照**key值**进行聚集。

#### reduce

对于reduce阶段，输入的<key,values>为

```
key：subject+class
values：某一科目某一班级对应的所有分数score的一个迭代器
```

```java
public static class MyReducer extends org.apache.hadoop.mapreduce.Reducer<Text, Text, Text, Text>{

    Text ValueOut = new Text();
    List<Integer> scoreList = new ArrayList<>();

    @Override
    protected void reduce(Text k2, Iterable<Text> v2s,
                          Reducer<Text, Text, Text, Text>.Context context) throws IOException, InterruptedException {
        //每次调用时首先清空scoreList
        scoreList.clear();
        for (Text v2:v2s){
            scoreList.add(Integer.valueOf(v2.toString()));
        }

        int sum_score = 0;
        for (int score:scoreList){
            sum_score += score;
        }

        double avg_score = sum_score * 1D / scoreList.size();

        ValueOut.set(String.valueOf(avg_score));
        context.write(k2,ValueOut);

    }
}
```

根据题目需求计算每一个科目每一班级的平均成绩，即对每一个key-value键值对，对value迭代求总和再取均值即可。

## 主要步骤和实验结果

### 主要步骤

#### 启动实验环境



![image-20220611213658846](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112137915.png)

通过ssh协议从本机（宿主机）远程登陆到hadoopspark_singlenode虚拟机：

![image-20220611213827777](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112138825.png)

启动HDFS分布式文件系统：

![image-20220611213936979](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112139027.png)

#### 任务一

切换到任务一mapreduce程序所在目录，进行编译打包：

![image-20220611214154757](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112141831.png)

将实验所需的数据上传至hdfs文件系统中：

![image-20220611214354492](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112143544.png)

运行mapreduce程序：

![image-20220611214558654](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112145697.png)

#### 任务二

切换到任务二mapreduce程序所在目录，进行编译打包：

![image-20220611215315041](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112153114.png)

腾空任务一结果占用的ouput文件夹：

![image-20220611215452769](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112154814.png)

运行mapreduce程序：

![image-20220611215519319](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112155378.png)

### 实验结果

#### 任务一

查看输出文件：

![image-20220611214948864](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112149911.png)

部分结果展示：

![image-20220611215043417](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112150462.png)

#### 任务二

查看输出文件：

![image-20220611215605711](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112156771.png)

部分结果展示：

![image-20220611215637459](https://cdn.jsdelivr.net/gh/mistletoe1222/img/imgs/202206112156504.png)



## 遇到的问题及解决方法

1. 在任务一完成后进行测试时，发现map过程中output：0 records；通过throws抛出错误的方法进行验证，发现`if (splited[3].equals("必修"))`这一if子句中的语句没有被执行，而且mvn package指令warning了部分内容的ACSII编码问题，怀疑是mvn的编码格式问题。

​		在pom.xml中添加了指定UTF-8格式编码，问题成功解决！

```xml
<properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
</properties>
```



## 心得体会

